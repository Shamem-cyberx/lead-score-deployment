<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>RAKEZ Case Study: Deploying and Monitoring a Lead Scoring Model</title>
    <style>
        @page {
            size: A4;
            margin: 2cm;
        }
        
        body {
            font-family: 'Segoe UI', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            page-break-after: avoid;
        }
        
        h2 {
            color: #34495e;
            border-bottom: 2px solid #95a5a6;
            padding-bottom: 8px;
            margin-top: 30px;
            page-break-after: avoid;
        }
        
        h3 {
            color: #555;
            margin-top: 20px;
            page-break-after: avoid;
        }
        
        code {
            background-color: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        
        pre {
            background-color: #f8f8f8;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            page-break-inside: avoid;
        }
        
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
            page-break-inside: avoid;
        }
        
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
        }
        
        th {
            background-color: #3498db;
            color: white;
        }
        
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        
        ul, ol {
            margin: 15px 0;
            padding-left: 30px;
        }
        
        li {
            margin: 8px 0;
        }
    </style>
</head>
<body>
    <h1>RAKEZ Case Study: Deploying and Monitoring a Lead Scoring Model</h1>
<p><strong>Role</strong>: Machine Learning Engineer<br />
<strong>Duration</strong>: 5-7 days<br />
<strong>Status</strong>: ✅ Complete</p>
<hr />
<h2>Executive Summary</h2>
<p>This document presents a comprehensive solution for deploying, monitoring, and maintaining RAKEZ's lead scoring model in a production environment. The solution addresses all requirements including deployment strategy, online testing, monitoring, automation, and retraining capabilities.</p>
<p><strong>Key Deliverables</strong>:<br />
- Production-ready deployment architecture<br />
- Online testing strategy (A/B testing and shadow deployment)<br />
- Comprehensive monitoring and alerting system<br />
- Automated retraining pipeline<br />
- Real-time monitoring dashboard<br />
- Complete documentation and presentation</p>
<hr />
<h2>1. Deployment Strategy</h2>
<h3>Architecture Overview</h3>
<p>The deployment architecture leverages <strong>Databricks</strong> for data processing and <strong>MLflow</strong> for model management, with a <strong>FastAPI</strong> REST API for real-time serving.</p>
<h3>Key Components</h3>
<h4>1.1 Model Registry (MLflow)</h4>
<p><strong>Implementation</strong>: <code>03_api/fastapi_app.py</code>, <code>02_notebooks/model_inference_databricks.py</code></p>
<ul>
<li><strong>Stages</strong>: Production, Staging, Archived</li>
<li><strong>Versioning</strong>: Automatic version tracking with metadata</li>
<li><strong>Auditability</strong>: Complete experiment tracking and model lineage</li>
<li><strong>Rollback</strong>: One-click reversion to previous versions</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># Model loading from registry</span>
<span class="n">model_uri</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;models:/</span><span class="si">{</span><span class="n">MODEL_NAME</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">STAGE</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">sklearn</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="p">)</span>
</code></pre></div>

<h4>1.2 REST API (FastAPI)</h4>
<p><strong>Implementation</strong>: <code>03_api/fastapi_app.py</code></p>
<ul>
<li><strong>Endpoint</strong>: <code>/score-lead</code> for real-time scoring</li>
<li><strong>Features</strong>:</li>
<li>Input validation (Pydantic models)</li>
<li>Shadow model support</li>
<li>Request logging</li>
<li>Error handling</li>
<li>Health checks</li>
</ul>
<h4>1.3 Batch Inference (Databricks)</h4>
<p><strong>Implementation</strong>: <code>02_notebooks/model_inference_databricks.py</code></p>
<ul>
<li>Scheduled daily batch jobs</li>
<li>Processes new leads from Delta Lake</li>
<li>Updates CRM via Delta tables</li>
<li>Handles feature engineering consistently</li>
</ul>
<h3>Deployment Methods</h3>
<ol>
<li>
<p><strong>Canary Deployment</strong><br />
   - Phase 1: 10% traffic (1 day)<br />
   - Phase 2: 50% traffic (2 days)<br />
   - Phase 3: 100% traffic<br />
   - Automatic rollback on failure</p>
</li>
<li>
<p><strong>Shadow Deployment</strong><br />
   - Parallel evaluation without affecting production<br />
   - Silent comparison of predictions<br />
   - Real-world performance data collection</p>
</li>
</ol>
<h3>Frameworks &amp; Tools</h3>
<ul>
<li>✅ <strong>MLflow</strong>: Model versioning and registry</li>
<li>✅ <strong>FastAPI</strong>: REST API framework</li>
<li>✅ <strong>Databricks</strong>: Data processing platform</li>
<li>✅ <strong>Delta Lake</strong>: Data storage and versioning</li>
</ul>
<hr />
<h2>2. Online Testing Approach</h2>
<h3>Strategy Overview</h3>
<p>We implement a <strong>two-phase testing approach</strong>: Shadow deployment followed by A/B testing.</p>
<h3>Phase 1: Shadow Deployment</h3>
<p><strong>Duration</strong>: 1-2 weeks</p>
<p><strong>Implementation</strong>: <code>03_api/fastapi_app.py</code> (shadow model support)</p>
<ul>
<li>New model runs in parallel with production</li>
<li>All traffic routed to both models</li>
<li>Predictions compared silently</li>
<li>Zero risk to business operations</li>
</ul>
<p><strong>Metrics Tracked</strong>:<br />
- Prediction distribution differences<br />
- Model performance (AUC, Precision, Recall)<br />
- Business KPIs (conversion rate, revenue)</p>
<h3>Phase 2: A/B Testing</h3>
<p><strong>Traffic Split</strong>: 90% production, 10% new model</p>
<p><strong>Success Criteria</strong>:<br />
- <strong>Technical</strong>: +2% AUC improvement OR<br />
- <strong>Business</strong>: +5% conversion rate OR +5% revenue per lead<br />
- <strong>Operational</strong>: No increase in error rate, latency maintained</p>
<p><strong>Decision Process</strong>:<br />
1. Monitor for statistical significance<br />
2. Compare business metrics<br />
3. Gather sales team feedback<br />
4. Gradual rollout if successful</p>
<h3>Safety Measures</h3>
<ul>
<li><strong>Automatic Rollback</strong>: Triggered if error rate &gt; 2% or latency degrades &gt; 50%</li>
<li><strong>Real-time Monitoring</strong>: Continuous tracking during testing</li>
<li><strong>Approval Gates</strong>: Manual review before full promotion</li>
<li><strong>Gradual Rollout</strong>: 10% → 50% → 100% traffic</li>
</ul>
<hr />
<h2>3. Monitoring Plan</h2>
<h3>3.1 Data Drift Detection</h3>
<p><strong>Implementation</strong>: <code>02_notebooks/drift_detection.py</code></p>
<h4>Metrics:</h4>
<ol>
<li>
<p><strong>PSI (Population Stability Index)</strong><br />
   - <strong>Calculation</strong>: Per-feature and overall<br />
   - <strong>Thresholds</strong>:</p>
<ul>
<li>PSI &lt; 0.1: No significant change</li>
<li>PSI 0.1-0.25: Moderate change (Warning)</li>
<li>PSI &gt; 0.25: Significant change (Critical)</li>
</ul>
</li>
<li>
<p><strong>KL Divergence</strong><br />
   - Measures distribution shift<br />
   - Threshold: KL &gt; 0.1 indicates drift</p>
</li>
<li>
<p><strong>Statistical Tests</strong><br />
   - <strong>Kolmogorov-Smirnov</strong>: Continuous features<br />
   - <strong>Chi-square</strong>: Categorical features<br />
   - P-value &lt; 0.05 indicates significant drift</p>
</li>
</ol>
<h4>Monitoring Schedule:</h4>
<ul>
<li><strong>Real-time</strong>: Feature statistics</li>
<li><strong>Hourly</strong>: Distribution checks</li>
<li><strong>Daily</strong>: PSI calculation</li>
<li><strong>Weekly</strong>: Comprehensive drift report</li>
</ul>
<h3>3.2 Prediction Drift</h3>
<p><strong>Implementation</strong>: <code>02_notebooks/monitoring_metrics.py</code></p>
<ul>
<li>Model performance degradation detection</li>
<li>AUC, Precision, Recall tracking over time</li>
<li>Calibration metrics (Brier score)</li>
</ul>
<h3>3.3 Latency &amp; Throughput</h3>
<p><strong>Metrics</strong>:<br />
- <strong>Latency</strong>: P50, P95, P99 percentiles<br />
  - Target: P95 &lt; 200ms<br />
  - Warning: P95 &gt; 500ms<br />
- <strong>Throughput</strong>: Requests per second<br />
  - Target: &gt; 10 req/s<br />
- <strong>Error Rate</strong>: HTTP errors<br />
  - Target: &lt; 1%<br />
  - Critical: &gt; 1%</p>
<h3>3.4 Business Performance</h3>
<p><strong>Metrics</strong>:<br />
- <strong>Conversion Rate</strong>: Leads → Customers<br />
- <strong>Revenue per Lead</strong>: Average revenue<br />
- <strong>Score Distribution</strong>: Lead score buckets<br />
- <strong>Conversion by Score</strong>: Performance by score category</p>
<h3>3.5 Alerting System</h3>
<p><strong>Implementation</strong>: <code>01_architecture/monitoring_architecture.md</code></p>
<p><strong>Alert Levels</strong>:<br />
- <strong>Level 1 (Info)</strong>: Logged to dashboard<br />
- <strong>Level 2 (Warning)</strong>: Slack notification (#ml-alerts)<br />
- <strong>Level 3 (Critical)</strong>: Email + Slack + PagerDuty</p>
<p><strong>Alert Triggers</strong>:<br />
- PSI &gt; 0.5: Critical drift<br />
- Latency P95 &gt; 500ms: Critical<br />
- Error rate &gt; 1%: Critical<br />
- Conversion rate drop &gt; 10%: Warning</p>
<h3>3.6 Sales Team Complaint Investigation</h3>
<p><strong>Scenario</strong>: "Model scores are no longer helping prioritize leads effectively"</p>
<p><strong>Investigation Workflow</strong> (6 Steps):</p>
<ol>
<li>
<p><strong>Immediate Response (1 hour)</strong><br />
   - Check model health metrics<br />
   - Verify API functionality<br />
   - Review recent model changes<br />
   - Check data quality issues</p>
</li>
<li>
<p><strong>Data Analysis (4 hours)</strong><br />
   - Analyze conversion rates by score bucket<br />
   - Compare current vs historical performance<br />
   - Check for data drift (PSI, KL divergence)<br />
   - Review feature distributions</p>
</li>
<li>
<p><strong>Model Performance Review (24 hours)</strong><br />
   - Evaluate model metrics (AUC, Precision, Recall)<br />
   - Compare production vs shadow model<br />
   - Review calibration plots<br />
   - Check for concept drift</p>
</li>
<li>
<p><strong>Root Cause Analysis</strong><br />
   - <strong>If Data Drift</strong>: Investigate data source changes<br />
   - <strong>If Concept Drift</strong>: Business environment may have changed<br />
   - <strong>If Model Issue</strong>: Review training data and features<br />
   - <strong>If Integration Issue</strong>: Check CRM sync and data pipeline</p>
</li>
<li>
<p><strong>Resolution</strong><br />
   - <strong>Data Issue</strong>: Fix data pipeline, retrain model<br />
   - <strong>Model Issue</strong>: Retrain with updated data/features<br />
   - <strong>Business Change</strong>: Update model to reflect new patterns<br />
   - <strong>Integration Issue</strong>: Fix CRM sync mechanism</p>
</li>
<li>
<p><strong>Communication &amp; Prevention</strong><br />
   - Document findings and resolution<br />
   - Update sales team with explanation<br />
   - Implement preventive measures<br />
   - Schedule follow-up review</p>
</li>
</ol>
<p><strong>Tools Used</strong>:<br />
- Streamlit dashboard for metrics<br />
- MLflow UI for model comparison<br />
- Drift detection reports<br />
- Conversion rate analysis</p>
<hr />
<h2>4. Automation, Reproducibility &amp; Retraining</h2>
<h3>4.1 Reproducibility</h3>
<p><strong>Implementation</strong>: MLflow + Delta Lake + Git</p>
<p><strong>Components</strong>:<br />
- <strong>MLflow</strong>: Experiment tracking, model versioning, parameter logging<br />
- <strong>Delta Lake</strong>: Data snapshots and versioning<br />
- <strong>Git</strong>: Code and configuration version control</p>
<p><strong>Data Snapshots</strong>:<br />
- Training data stored in Delta Lake with timestamps<br />
- Feature engineering code versioned<br />
- Model artifacts stored in MLflow</p>
<h3>4.2 CI/CD Workflow</h3>
<p><strong>Implementation</strong>: <code>04_ci_cd/github_actions.yaml</code></p>
<p><strong>Pipeline Stages</strong>:</p>
<ol>
<li>
<p><strong>Lint &amp; Test</strong><br />
   - Code formatting (Black)<br />
   - Linting (Flake8)<br />
   - Unit tests (Pytest)</p>
</li>
<li>
<p><strong>Validate</strong><br />
   - Notebook syntax validation<br />
   - Model validation checks</p>
</li>
<li>
<p><strong>Deploy Staging</strong><br />
   - Auto-deploy on develop branch<br />
   - Deploy notebooks to Databricks<br />
   - Trigger inference job</p>
</li>
<li>
<p><strong>Deploy Production</strong><br />
   - Manual approval required<br />
   - Canary deployment (10% → 50% → 100%)<br />
   - Monitor metrics<br />
   - Automatic rollback on failure</p>
</li>
<li>
<p><strong>Model Registry Update</strong><br />
   - Promote model from Staging to Production<br />
   - Update model tags and metadata</p>
</li>
</ol>
<h3>4.3 Retraining Strategy</h3>
<p><strong>Implementation</strong>: <code>02_notebooks/retraining_pipeline.py</code></p>
<h4>Triggers:</h4>
<ol>
<li>
<p><strong>Drift-Triggered</strong><br />
   - Automatic when PSI &gt; 0.25<br />
   - Significant distribution shift detected</p>
</li>
<li>
<p><strong>Scheduled</strong><br />
   - Weekly for first 3 months<br />
   - Monthly thereafter</p>
</li>
<li>
<p><strong>Manual</strong><br />
   - Admin-initiated for special cases</p>
</li>
</ol>
<h4>Retraining Process:</h4>
<ol>
<li>
<p><strong>Data Collection</strong><br />
   - Latest 6 months of labeled data<br />
   - Minimum 10,000 records required<br />
   - Time-based train/test split</p>
</li>
<li>
<p><strong>Model Training</strong><br />
   - Hyperparameter optimization (Optuna, 50-100 trials)<br />
   - Time series cross-validation<br />
   - XGBoost/LightGBM models</p>
</li>
<li>
<p><strong>Model Evaluation</strong><br />
   - Must outperform production model<br />
   - Minimum improvement: +2% AUC OR +5% business KPI<br />
   - Shadow testing for 1-2 weeks</p>
</li>
<li>
<p><strong>Model Promotion</strong><br />
   - Register to MLflow Staging<br />
   - Manual review and approval<br />
   - Canary deployment<br />
   - Promote to Production</p>
</li>
</ol>
<h4>Rollback Mechanism:</h4>
<ul>
<li><strong>Automatic</strong>: Error rate &gt; 2%, Latency &gt; 50% degradation, Business KPI drop &gt; 10%</li>
<li><strong>Manual</strong>: One-click rollback to previous version</li>
<li><strong>History</strong>: Maintains model registry history</li>
</ul>
<hr />
<h2>5. Bonus Features</h2>
<h3>5.1 Monitoring Dashboard</h3>
<p><strong>Implementation</strong>: <code>05_dashboard/streamlit_dashboard.py</code></p>
<p><strong>Features</strong>:<br />
- Real-time metrics visualization<br />
- Drift detection charts (PSI by feature)<br />
- Performance trends (latency, throughput)<br />
- Business metrics (conversion rates by score bucket)<br />
- Alert viewer<br />
- Works in demo mode (mock data) and production mode</p>
<p><strong>Tabs</strong>:<br />
1. <strong>Overview</strong>: System metrics and performance trends<br />
2. <strong>Drift Detection</strong>: PSI and distribution monitoring<br />
3. <strong>Performance</strong>: Latency and throughput metrics<br />
4. <strong>Business Metrics</strong>: Conversion rates and score distributions<br />
5. <strong>Alerts</strong>: System alerts and notifications</p>
<h3>5.2 CRM Integration</h3>
<p><strong>Implementation</strong>: <code>06_docs/detailed_readme.md</code> (CRM Integration Plan)</p>
<p><strong>3-Phase Approach</strong>:</p>
<p><strong>Phase 1: Batch Integration</strong><br />
- Scheduled job updates CRM scores daily<br />
- Delta table as intermediary<br />
- Low latency requirement</p>
<p><strong>Phase 2: Real-time Integration</strong><br />
- FastAPI sends webhook on prediction<br />
- CRM receives score immediately<br />
- Higher latency requirement</p>
<p><strong>Phase 3: API Integration</strong><br />
- CRM calls FastAPI directly<br />
- On-demand scoring<br />
- Highest latency requirement</p>
<p><strong>Integration Methods</strong>:<br />
- Webhook integration<br />
- API integration<br />
- Database integration (Delta tables)</p>
<h3>5.3 Feedback Loop</h3>
<p><strong>Implementation</strong>: <code>06_docs/presentation_slides.md</code> (Slide 10)</p>
<ul>
<li>Sales team complaint investigation workflow</li>
<li>Root cause analysis process</li>
<li>Preventive measures implementation</li>
<li>Regular stakeholder communication</li>
</ul>
<hr />
<h2>Technical Implementation</h2>
<h3>Code Structure</h3>
<div class="codehilite"><pre><span></span><code>rakez-lead-scoring-deployment/
├── 01_architecture/          # Architecture diagrams
├── 02_notebooks/             # Databricks notebooks
├── 03_api/                   # FastAPI application
├── 04_ci_cd/                 # CI/CD pipeline
├── 05_dashboard/             # Monitoring dashboard
└── 06_docs/                  # Documentation
</code></pre></div>

<h3>Key Technologies</h3>
<ul>
<li><strong>ML</strong>: XGBoost, LightGBM, scikit-learn</li>
<li><strong>Platform</strong>: Databricks, MLflow</li>
<li><strong>API</strong>: FastAPI, Uvicorn</li>
<li><strong>Dashboard</strong>: Streamlit, Plotly</li>
<li><strong>CI/CD</strong>: GitHub Actions</li>
<li><strong>Data</strong>: Delta Lake, Spark</li>
</ul>
<hr />
<h2>Conclusion</h2>
<p>This solution provides a <strong>complete, production-ready system</strong> for deploying and monitoring RAKEZ's lead scoring model. All assessment requirements have been met:</p>
<p>✅ <strong>Deployment Strategy</strong>: Complete with MLflow, FastAPI, and Databricks<br />
✅ <strong>Online Testing</strong>: Shadow deployment and A/B testing implemented<br />
✅ <strong>Monitoring Plan</strong>: Comprehensive drift detection, performance metrics, and alerting<br />
✅ <strong>Automation &amp; Retraining</strong>: CI/CD pipeline and automated retraining<br />
✅ <strong>Bonus Features</strong>: Working dashboard, CRM integration, feedback loop  </p>
<p>The solution is <strong>enterprise-grade</strong>, <strong>well-documented</strong>, and <strong>ready for production deployment</strong>.</p>
<hr />
<h2>Appendix</h2>
<h3>Files and Locations</h3>
<ul>
<li><strong>Architecture Diagrams</strong>: <code>01_architecture/</code></li>
<li><strong>Production Code</strong>: <code>02_notebooks/</code>, <code>03_api/</code></li>
<li><strong>CI/CD Pipeline</strong>: <code>04_ci_cd/github_actions.yaml</code></li>
<li><strong>Dashboard</strong>: <code>05_dashboard/streamlit_dashboard.py</code></li>
<li><strong>Documentation</strong>: <code>06_docs/</code></li>
</ul>
<h3>Quick Start</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># Setup</span>
python<span class="w"> </span>setup.py

<span class="c1"># Start services</span>
python<span class="w"> </span>start.py

<span class="c1"># Access</span>
<span class="c1"># - API: http://localhost:8000/docs</span>
<span class="c1"># - Dashboard: http://localhost:8501</span>
</code></pre></div>

<hr />
<p><strong>End of Case Study</strong></p>
</body>
</html>